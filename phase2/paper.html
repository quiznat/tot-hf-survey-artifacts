<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Tree-of-Thought Search with Hugging Face Inference Models</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="./manuscript/phase2-paper.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Tree-of-Thought Search with Hugging Face Inference
Models</h1>
<p class="subtitle">Phase 2 Prepaper (Living Draft)</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#phase-2-prepaper-living-source-of-truth"
id="toc-phase-2-prepaper-living-source-of-truth"><span
class="toc-section-number">1</span> Phase 2 Prepaper (Living Source of
Truth)</a>
<ul>
<li><a href="#working-title" id="toc-working-title"><span
class="toc-section-number">1.1</span> Working Title</a></li>
<li><a href="#scope" id="toc-scope"><span
class="toc-section-number">1.2</span> Scope</a></li>
<li><a href="#protocol-freeze-v2-approved"
id="toc-protocol-freeze-v2-approved"><span
class="toc-section-number">1.3</span> Protocol Freeze v2
(Approved)</a></li>
<li><a href="#methodology-decision-freeze-2026-02-20"
id="toc-methodology-decision-freeze-2026-02-20"><span
class="toc-section-number">1.4</span> Methodology Decision Freeze
(2026-02-20)</a></li>
<li><a href="#research-questions" id="toc-research-questions"><span
class="toc-section-number">1.5</span> Research Questions</a></li>
<li><a href="#experimental-design-current"
id="toc-experimental-design-current"><span
class="toc-section-number">1.6</span> Experimental Design
(Current)</a></li>
<li><a href="#statistical-plan-current"
id="toc-statistical-plan-current"><span
class="toc-section-number">1.7</span> Statistical Plan
(Current)</a></li>
<li><a href="#draft-manuscript-text-methods-and-experimental-setup-v01"
id="toc-draft-manuscript-text-methods-and-experimental-setup-v01"><span
class="toc-section-number">1.8</span> Draft Manuscript Text: Methods and
Experimental Setup (v0.1)</a>
<ul>
<li><a href="#4-methods" id="toc-4-methods"><span
class="toc-section-number">1.8.1</span> 4. Methods</a></li>
<li><a href="#5-experimental-setup" id="toc-5-experimental-setup"><span
class="toc-section-number">1.8.2</span> 5. Experimental Setup</a></li>
</ul></li>
<li><a href="#draft-manuscript-text-results-and-limitations-v01"
id="toc-draft-manuscript-text-results-and-limitations-v01"><span
class="toc-section-number">1.9</span> Draft Manuscript Text: Results and
Limitations (v0.1)</a>
<ul>
<li><a href="#6-results" id="toc-6-results"><span
class="toc-section-number">1.9.1</span> 6. Results</a></li>
<li><a href="#7-limitations" id="toc-7-limitations"><span
class="toc-section-number">1.9.2</span> 7. Limitations</a></li>
</ul></li>
<li><a href="#executed-pilot-evidence-current"
id="toc-executed-pilot-evidence-current"><span
class="toc-section-number">1.10</span> Executed Pilot Evidence
(Current)</a></li>
<li><a href="#executed-lockset-evidence-50-paired-items"
id="toc-executed-lockset-evidence-50-paired-items"><span
class="toc-section-number">1.11</span> Executed Lockset Evidence (50
Paired Items)</a></li>
<li><a href="#executed-protocol-v2-matrix-evidence-3-locked-models"
id="toc-executed-protocol-v2-matrix-evidence-3-locked-models"><span
class="toc-section-number">1.12</span> Executed Protocol-v2 Matrix
Evidence (3 Locked Models)</a></li>
<li><a href="#executed-evaluator-ablation-evidence-primary-model"
id="toc-executed-evaluator-ablation-evidence-primary-model"><span
class="toc-section-number">1.13</span> Executed Evaluator Ablation
Evidence (Primary Model)</a></li>
<li><a href="#executed-search-ablation-evidence-primary-model"
id="toc-executed-search-ablation-evidence-primary-model"><span
class="toc-section-number">1.14</span> Executed Search Ablation Evidence
(Primary Model)</a></li>
<li><a href="#claim-boundary" id="toc-claim-boundary"><span
class="toc-section-number">1.15</span> Claim Boundary</a></li>
<li><a href="#prepaper-build-plan" id="toc-prepaper-build-plan"><span
class="toc-section-number">1.16</span> Prepaper Build Plan</a></li>
<li><a href="#required-tablesfigures-planned"
id="toc-required-tablesfigures-planned"><span
class="toc-section-number">1.17</span> Required Tables/Figures
(Planned)</a></li>
<li><a href="#protocol-changes-log" id="toc-protocol-changes-log"><span
class="toc-section-number">1.18</span> Protocol Changes Log</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="phase-2-prepaper-living-source-of-truth"><span
class="header-section-number">1</span> Phase 2 Prepaper (Living Source
of Truth)</h1>
<p>Status: active draft (build-as-we-go) Last updated: 2026-02-21</p>
<h2 data-number="1.1" id="working-title"><span
class="header-section-number">1.1</span> Working Title</h2>
<p>Tree-of-Thought Search with Hugging Face Inference Models:
Reproducible Evaluation with LLM-Based In-Chain Judging</p>
<h2 data-number="1.2" id="scope"><span
class="header-section-number">1.2</span> Scope</h2>
<p>This prepaper is the canonical source for Phase 2 methodological
decisions, frozen protocol settings, claim boundaries, and evolving
manuscript text.</p>
<h2 data-number="1.3" id="protocol-freeze-v2-approved"><span
class="header-section-number">1.3</span> Protocol Freeze v2
(Approved)</h2>
<ul>
<li>Active protocol file:
<code>phase2/benchmarks/evaluation-protocol-v2.md</code></li>
<li>Protocol ID: <code>TOT-HF-P2-EPV2-2026-02-20</code></li>
<li>Freeze date: 2026-02-20</li>
</ul>
<h2 data-number="1.4" id="methodology-decision-freeze-2026-02-20"><span
class="header-section-number">1.4</span> Methodology Decision Freeze
(2026-02-20)</h2>
<ul>
<li>Primary ToT methodology uses LLM-based in-chain evaluation
(<code>model_self_eval</code>).</li>
<li><code>rule_based</code> and <code>hybrid</code> evaluator modes are
retained for ablations/controls only.</li>
<li>For Game24, arithmetic correctness checks are used as an offline
measurement oracle for reporting metrics, not as the primary in-chain
decision mechanism.</li>
<li>No performance claim is made without paired-condition evidence on
the same benchmark item set.</li>
</ul>
<h2 data-number="1.5" id="research-questions"><span
class="header-section-number">1.5</span> Research Questions</h2>
<ul>
<li>RQ1: Does ToT with LLM in-chain evaluation improve success over
single-path and ReAct baselines on matched item sets?</li>
<li>RQ2: What are latency/token tradeoffs of ToT under fixed budget
constraints?</li>
<li>RQ3: How sensitive are outcomes to evaluator mode and search policy
choices?</li>
</ul>
<h2 data-number="1.6" id="experimental-design-current"><span
class="header-section-number">1.6</span> Experimental Design
(Current)</h2>
<ul>
<li>Conditions:
<ul>
<li><code>baseline-single-path</code></li>
<li><code>baseline-react</code></li>
<li><code>tot-prototype</code> (default evaluator mode:
<code>model_self_eval</code>)</li>
</ul></li>
<li>Provider/model:
<ul>
<li>Hugging Face Router</li>
<li>Primary model for current lock set:
<code>Qwen/Qwen3-Coder-Next:novita</code></li>
</ul></li>
<li>Task family:
<ul>
<li>Game24 arithmetic expression synthesis</li>
</ul></li>
<li>Pairing policy:
<ul>
<li>Identical item IDs evaluated across all conditions.</li>
</ul></li>
</ul>
<h2 data-number="1.7" id="statistical-plan-current"><span
class="header-section-number">1.7</span> Statistical Plan (Current)</h2>
<ul>
<li>Fixed panel: 50 paired Game24 items per condition (150 total runs
per model for primary 3-condition matrix).</li>
<li>Primary success analysis:
<ul>
<li>paired condition comparisons on identical item IDs,</li>
<li>Wilson CIs for condition success rates,</li>
<li>paired bootstrap percentile CIs for success-rate deltas,</li>
<li>exact McNemar p-values with Holm correction.</li>
</ul></li>
<li>Interpretation guardrail:
<ul>
<li>claim scope is panel/model-specific unless replicated across
additional tasks/models.</li>
</ul></li>
</ul>
<h2 data-number="1.8"
id="draft-manuscript-text-methods-and-experimental-setup-v01"><span
class="header-section-number">1.8</span> Draft Manuscript Text: Methods
and Experimental Setup (v0.1)</h2>
<h3 data-number="1.8.1" id="4-methods"><span
class="header-section-number">1.8.1</span> 4. Methods</h3>
<p>We evaluate Tree-of-Thought (ToT) search in a controlled agent
harness with three conditions implemented under a shared execution
stack: <code>baseline-single-path</code>, <code>baseline-react</code>,
and <code>tot-prototype</code>. The single-path baseline performs one
forward generation pass and is scored on the final extracted expression.
The ReAct baseline runs an iterative loop (<code>max_steps=5</code>)
with a <code>calc</code> tool and returns either a tagged
<code>FINAL</code> answer or a recovered action expression when valid.
The ToT prototype maintains an explicit frontier of thought nodes. At
each depth, it generates candidate expressions, evaluates each
candidate, accumulates scores along parent-child paths, prunes to a
fixed frontier width by cumulative score, and stops on the first
terminal solution or at depth limit.</p>
<p>The primary ToT evaluator is <code>model_self_eval</code>, where the
same model assigns a scalar score in <code>[0,1]</code> to each
candidate via an in-chain scoring prompt. <code>rule_based</code> and
<code>hybrid</code> evaluators are retained as control/ablation modes
only and are not used for primary claims. Default primary search
settings are <code>max_depth=3</code>, <code>branch_factor=3</code>, and
<code>frontier_width=3</code>, with frozen ablation presets A1
(<code>2/3/3</code>) and A2 (<code>3/4/4</code>).</p>
<p>All runs use the Hugging Face Inference Router with fixed execution
controls from the frozen protocol
(<code>TOT-HF-P2-EPV2-2026-02-20</code>): deterministic item-level
seeding via <code>seed_policy=item_hash</code>, sampling controls logged
per run (<code>hf_temperature=0.0</code>, <code>hf_top_p=1.0</code>),
and no within-matrix model substitution. Each run emits a structured
manifest with run metadata (<code>run_id</code>, timestamp, condition,
provider/model, panel/item IDs, input), configuration fields (search and
evaluator settings), and outcome/trace metrics (success, latency,
tokens, notes, error type where applicable).</p>
<h3 data-number="1.8.2" id="5-experimental-setup"><span
class="header-section-number">1.8.2</span> 5. Experimental Setup</h3>
<p>The primary task is Game24 expression synthesis on a fixed paired
panel (<code>game24_lockset_v1</code>) containing 50 solvable items
(<code>selection_seed=20260220</code>, number range 1-10). Each item is
defined by four integers and an oracle solution string used for panel
provenance; agent execution receives only the four-number input. Success
is binary and requires an expression that uses each provided number
exactly once and evaluates to 24 under the task validator.</p>
<p>The locked model matrix is:</p>
<ul>
<li><code>Qwen/Qwen3-Coder-Next:novita</code></li>
<li><code>Qwen/Qwen2.5-72B-Instruct</code></li>
<li><code>Qwen/Qwen2.5-Coder-32B-Instruct</code></li>
</ul>
<p>For each model, we run all three primary conditions on the identical
50-item panel (<code>3 x 50 = 150</code> runs/model; <code>450</code>
runs total for the matrix). Additional ablations are executed on the
primary model (<code>Qwen/Qwen3-Coder-Next:novita</code>) using the same
paired panel design: evaluator-mode ablations
(<code>model_self_eval</code>, <code>rule_based</code>,
<code>hybrid</code>) and search-policy ablations (primary, A1, A2).
These executed batches contribute an additional <code>600</code> runs,
yielding <code>1050</code> protocol-v2 matrix+ablation runs in total
(excluding smoke prechecks).</p>
<p>Primary endpoint is success rate by condition. Secondary endpoints
are latency (ms) and token usage (<code>tokens_in</code>,
<code>tokens_out</code>). Statistical reporting is pre-registered in the
protocol and implemented by the lockset report pipeline: Wilson
confidence intervals for condition success rates, paired bootstrap
percentile confidence intervals for success-rate deltas
(<code>bootstrap_samples=10000</code>, fixed seed), and exact two-sided
McNemar tests for paired contrasts with Holm correction across reported
pairwise tests. All claims are restricted to panel/model-scoped
observations under this protocol.</p>
<h2 data-number="1.9"
id="draft-manuscript-text-results-and-limitations-v01"><span
class="header-section-number">1.9</span> Draft Manuscript Text: Results
and Limitations (v0.1)</h2>
<h3 data-number="1.9.1" id="6-results"><span
class="header-section-number">1.9.1</span> 6. Results</h3>
<p>Under the frozen protocol (<code>TOT-HF-P2-EPV2-2026-02-20</code>),
ToT with <code>model_self_eval</code> outperformed both baseline
conditions on all three locked models in paired 50-item Game24
evaluations. Success rates for <code>tot-prototype</code> were
<code>0.760</code> (<code>Qwen3-Coder-Next</code>), <code>0.580</code>
(<code>Qwen2.5-72B-Instruct</code>), and <code>0.680</code>
(<code>Qwen2.5-Coder-32B-Instruct</code>). Corresponding absolute deltas
versus ReAct were <code>+0.320</code>, <code>+0.560</code>, and
<code>+0.620</code>, with Holm-corrected McNemar p-values
<code>8.55e-04</code>, <code>2.24e-08</code>, and <code>2.79e-09</code>,
respectively. Deltas versus single-path were <code>+0.680</code>,
<code>+0.420</code>, and <code>+0.500</code>, each with corrected
p-values below <code>1e-04</code>.</p>
<p>Latency and token footprints indicate a cost-performance tradeoff
rather than uniform dominance. In the matrix reports, ToT latency
exceeds single-path latency on all models and usually exceeds ReAct
latency; however, magnitude varies by model and run window (for example,
near parity between ToT and ReAct on
<code>Qwen2.5-Coder-32B-Instruct</code>). This supports a scoped claim:
paired success gains are robust on the fixed panel, while compute
overhead is model- and configuration-dependent.</p>
<p>Evaluator ablations on the primary model
(<code>Qwen3-Coder-Next</code>) show that ToT remains superior to ReAct
across all tested evaluator modes: <code>model_self_eval</code>
(<code>0.760</code>), <code>rule_based</code> (<code>0.860</code>), and
<code>hybrid</code> (<code>0.780</code>) ToT success rates, each with
corrected paired significance. Because <code>rule_based</code> uses
task-structured heuristics, it is treated as a diagnostic control and
not the primary claim path. Search-policy ablations further show
sensitivity to depth/width settings: A1 (<code>2/3/3</code>) achieved
<code>0.720</code> ToT success, primary (<code>3/3/3</code>)
<code>0.760</code>, and A2 (<code>3/4/4</code>) <code>0.920</code>, with
ToT-vs-ReAct paired significance preserved in all presets.</p>
<p>Failure taxonomy analysis across archived protocol runs
(<code>n=202</code> failures) identifies dominant error classes:
<code>format_or_notation_mismatch</code> (<code>109</code>),
<code>other_failure</code> (<code>66</code>), and
<code>depth_limit_no_solution</code> (<code>24</code>), plus infrequent
<code>invalid_candidate_retained</code> (<code>2</code>) and
<code>unsafe_expression_filtered</code> (<code>1</code>). These patterns
are consistent with parser/normalization fragility and residual search
termination failures, and motivate targeted robustness work in output
normalization and candidate filtering.</p>
<h3 data-number="1.9.2" id="7-limitations"><span
class="header-section-number">1.9.2</span> 7. Limitations</h3>
<p>First, the current evidence is task-specific: all primary claims are
derived from a fixed Game24 panel and do not establish generalization to
broader tool-use or open-domain planning tasks. Second, the model set is
intentionally locked and relatively narrow; conclusions are limited to
the three evaluated models and the execution windows in which they were
run. Third, provider-side effects (routing, transient latency variance,
serving updates) may influence latency comparisons across batches even
when protocol controls are held fixed.</p>
<p>Fourth, the primary in-chain evaluator (<code>model_self_eval</code>)
uses the same model family for generation and scoring, which may
introduce correlated biases in branch selection. The offline arithmetic
validator provides an external correctness check for reporting, but does
not remove all possible in-chain evaluator biases. Fifth, some ablation
outcomes (for example, strong <code>rule_based</code> performance) may
reflect task-structured advantages and should not be interpreted as a
general recommendation for unconstrained domains.</p>
<p>Finally, this phase emphasizes internal reproducibility and
controlled paired inference rather than deployment realism: no external
human preference judgments, end-to-end agent productivity tasks, or
production cost audits are included yet. These omissions are intentional
and define the boundary of current claims.</p>
<h2 data-number="1.10" id="executed-pilot-evidence-current"><span
class="header-section-number">1.10</span> Executed Pilot Evidence
(Current)</h2>
<ul>
<li>Paired 3-item smoke panel executed across
<code>baseline-single-path</code>, <code>baseline-react</code>, and
<code>tot-prototype</code>.</li>
<li>Run settings:
<ul>
<li>provider/model: Hugging Face Router,
<code>Qwen/Qwen3-Coder-Next:novita</code></li>
<li>ToT evaluator mode: <code>model_self_eval</code></li>
</ul></li>
<li>Pilot artifact:
<ul>
<li><code>phase2/benchmarks/analysis/game24_lockset_report_pilot.md</code></li>
</ul></li>
<li>Pilot summary (exploratory only, not for final claims):
<ul>
<li><code>baseline-single-path</code>: 0/3</li>
<li><code>baseline-react</code>: 1/3</li>
<li><code>tot-prototype</code>: 2/3</li>
</ul></li>
<li>Interpretation:
<ul>
<li>confirms paired-run pipeline and reporting integrity,</li>
<li>insufficient sample for significance; full 50-item panel
required.</li>
</ul></li>
</ul>
<h2 data-number="1.11"
id="executed-lockset-evidence-50-paired-items"><span
class="header-section-number">1.11</span> Executed Lockset Evidence (50
Paired Items)</h2>
<ul>
<li>Full paired panel executed: 50 items x 3 conditions = 150 runs.</li>
<li>Full report artifacts:
<ul>
<li><code>phase2/benchmarks/analysis/game24_lockset_report.md</code></li>
<li><code>phase2/benchmarks/analysis/game24_lockset_report.json</code></li>
</ul></li>
<li>Condition-level outcomes on
<code>Qwen/Qwen3-Coder-Next:novita</code>:
<ul>
<li><code>baseline-single-path</code>: success 0.080, mean latency
6040.7 ms</li>
<li><code>baseline-react</code>: success 0.400, mean latency 21117.0
ms</li>
<li><code>tot-prototype (model_self_eval)</code>: success 0.840, mean
latency 53823.3 ms</li>
</ul></li>
<li>Paired deltas (A-B)/N:
<ul>
<li><code>baseline-react</code> vs <code>baseline-single-path</code>:
+0.320</li>
<li><code>tot-prototype</code> vs <code>baseline-react</code>: +0.440
(reported as -0.440 in reverse direction)</li>
<li><code>tot-prototype</code> vs <code>baseline-single-path</code>:
+0.760 (reported as -0.760 in reverse direction)</li>
</ul></li>
<li>Paired significance highlights:
<ul>
<li><code>baseline-react</code> vs <code>baseline-single-path</code>:
p=0.00154 (Holm 0.00154)</li>
<li><code>tot-prototype</code> vs <code>baseline-react</code>:
p=2.74e-05 (Holm 5.49e-05)</li>
<li><code>tot-prototype</code> vs <code>baseline-single-path</code>:
p=7.28e-12 (Holm 2.18e-11)</li>
</ul></li>
<li>Interpretation:
<ul>
<li>lockset execution confirms strong performance separation under this
fixed model/panel,</li>
<li>ToT gains are accompanied by higher latency/token usage and must be
discussed as an explicit tradeoff.</li>
</ul></li>
</ul>
<h2 data-number="1.12"
id="executed-protocol-v2-matrix-evidence-3-locked-models"><span
class="header-section-number">1.12</span> Executed Protocol-v2 Matrix
Evidence (3 Locked Models)</h2>
<ul>
<li>Locked models:
<ul>
<li><code>Qwen/Qwen3-Coder-Next:novita</code></li>
<li><code>Qwen/Qwen2.5-72B-Instruct</code></li>
<li><code>Qwen/Qwen2.5-Coder-32B-Instruct</code></li>
</ul></li>
<li>Matrix summary artifacts:
<ul>
<li><code>phase2/benchmarks/analysis/game24_lockset_matrix_summary_protocol_v2.md</code></li>
<li><code>phase2/benchmarks/analysis/game24_lockset_matrix_summary_protocol_v2.json</code></li>
</ul></li>
<li>Success-rate snapshot across conditions:
<ul>
<li><code>Qwen/Qwen3-Coder-Next:novita</code>: single 0.080, react
0.440, tot 0.760</li>
<li><code>Qwen/Qwen2.5-72B-Instruct</code>: single 0.160, react 0.020,
tot 0.580</li>
<li><code>Qwen/Qwen2.5-Coder-32B-Instruct</code>: single 0.180, react
0.060, tot 0.680</li>
</ul></li>
<li>Interpretation:
<ul>
<li>ToT outperforms ReAct and single-path on all three locked models
under paired evaluation.</li>
<li>ReAct underperforms markedly on two locked models, suggesting strong
model-format sensitivity in the baseline condition.</li>
</ul></li>
</ul>
<h2 data-number="1.13"
id="executed-evaluator-ablation-evidence-primary-model"><span
class="header-section-number">1.13</span> Executed Evaluator Ablation
Evidence (Primary Model)</h2>
<ul>
<li>Primary model: <code>Qwen/Qwen3-Coder-Next:novita</code></li>
<li>Evaluator modes executed on identical 50-item paired panel:
<ul>
<li><code>model_self_eval</code>: single 0.080, react 0.440, tot
0.760</li>
<li><code>rule_based</code>: single 0.080, react 0.420, tot 0.860</li>
<li><code>hybrid</code>: single 0.080, react 0.420, tot 0.780</li>
</ul></li>
<li>Consolidated artifacts:
<ul>
<li><code>phase2/benchmarks/analysis/game24_lockset_evaluator_ablation_summary.md</code></li>
<li><code>phase2/benchmarks/analysis/game24_lockset_evaluator_ablation_summary.json</code></li>
</ul></li>
<li>Significance snapshot (ToT vs ReAct, paired McNemar):
<ul>
<li><code>model_self_eval</code>: p=8.55e-04</li>
<li><code>rule_based</code>: p=2.74e-05</li>
<li><code>hybrid</code>: p=4.01e-05</li>
</ul></li>
<li>Interpretation:
<ul>
<li>ToT remains superior to both baselines under all evaluator modes on
this fixed panel.</li>
<li><code>model_self_eval</code> remains the primary methodology;
<code>rule_based</code>/<code>hybrid</code> remain ablation/control
conditions.</li>
</ul></li>
</ul>
<h2 data-number="1.14"
id="executed-search-ablation-evidence-primary-model"><span
class="header-section-number">1.14</span> Executed Search Ablation
Evidence (Primary Model)</h2>
<ul>
<li>Primary model: <code>Qwen/Qwen3-Coder-Next:novita</code></li>
<li>Search presets executed on identical 50-item paired panel:
<ul>
<li><code>primary</code> (<code>depth=3, branch=3, frontier=3</code>):
single 0.080, react 0.440, tot 0.760</li>
<li><code>A1</code> (<code>depth=2, branch=3, frontier=3</code>): single
0.060, react 0.360, tot 0.720</li>
<li><code>A2</code> (<code>depth=3, branch=4, frontier=4</code>): single
0.080, react 0.420, tot 0.920</li>
</ul></li>
<li>Consolidated artifacts:
<ul>
<li><code>phase2/benchmarks/analysis/game24_lockset_search_ablation_summary.md</code></li>
<li><code>phase2/benchmarks/analysis/game24_lockset_search_ablation_summary.json</code></li>
</ul></li>
<li>Significance snapshot (ToT vs ReAct, paired McNemar):
<ul>
<li><code>primary</code>: p=8.55e-04 (Holm 8.55e-04)</li>
<li><code>A1</code>: p=1.21e-04 (Holm 2.42e-04)</li>
<li><code>A2</code>: p=5.96e-08 (Holm 1.19e-07)</li>
</ul></li>
<li>ToT latency/token snapshot by preset:
<ul>
<li><code>primary</code>: 58213.6 ms, tokens_in 978.3, tokens_out
107.0</li>
<li><code>A1</code>: 9050.9 ms, tokens_in 636.7, tokens_out 70.8</li>
<li><code>A2</code>: 11262.8 ms, tokens_in 955.9, tokens_out 121.9</li>
</ul></li>
<li>Interpretation:
<ul>
<li>ToT remains superior to ReAct under all three search presets on this
panel/model.</li>
<li>A2 currently yields the strongest accuracy (0.920) with markedly
lower observed latency than the earlier <code>primary</code> execution
window; this should be treated as observed run behavior under provider
conditions, not a universal latency law.</li>
<li>Search policy materially affects both outcome quality and compute
profile, and must remain explicitly reported in all claims.</li>
</ul></li>
</ul>
<h2 data-number="1.15" id="claim-boundary"><span
class="header-section-number">1.15</span> Claim Boundary</h2>
<ul>
<li>Allowed claim pattern: "On fixed paired Game24 items, condition A
outperformed condition B by X absolute success points under protocol
Y."</li>
<li>Disallowed claim pattern: broad generalization to arbitrary tasks or
models from single-task pilot results.</li>
</ul>
<h2 data-number="1.16" id="prepaper-build-plan"><span
class="header-section-number">1.16</span> Prepaper Build Plan</h2>
<ol type="1">
<li>Draft Methods and Experimental Setup directly from frozen protocol
and executed manifests. (completed in v0.1 section above)</li>
<li>Draft Results, Failure Analysis, and Tradeoff analysis from archived
artifacts only. (completed in v0.1 section above)</li>
<li>Draft Limitations and Threats to Validity before conclusion text.
(limitations completed in v0.1 section above; dedicated
threats-to-validity consolidation remains in final polish)</li>
<li>Prepare reproducibility appendix with final command blocks and
artifact index.</li>
<li>Build anonymous manuscript package for first submission cycle.</li>
</ol>
<p>Canonical execution commands for search ablations are archived
in:</p>
<ul>
<li><code>phase2/benchmarks/protocol-v2-search-ablation-execution.md</code></li>
</ul>
<h2 data-number="1.17" id="required-tablesfigures-planned"><span
class="header-section-number">1.17</span> Required Tables/Figures
(Planned)</h2>
<ul>
<li>Table 1: Condition-level success/latency/token metrics (paired
panel).</li>
<li>Table 2: Ablation summary (evaluator mode and search presets).</li>
<li>Table 3: Failure taxonomy with representative run IDs.</li>
<li>Figure 1: ToT search-state trace diagram with stop-policy
points.</li>
</ul>
<h2 data-number="1.18" id="protocol-changes-log"><span
class="header-section-number">1.18</span> Protocol Changes Log</h2>
<ul>
<li>2026-02-20: Set LLM-based in-chain evaluation as primary
methodology; moved rule-based scoring to ablation/control role.</li>
<li>2026-02-20: Added fixed 50-item Game24 paired panel
(<code>game24_lockset_v1</code>) and lockset runner script.</li>
<li>2026-02-20: Completed 3-item paired smoke pilot; validated
end-to-end artifact/report pipeline.</li>
<li>2026-02-20: Completed full 50-item paired lockset execution and
archived final report artifacts.</li>
<li>2026-02-20: Added inferential statistics to lockset reporting (CI +
exact McNemar + Holm correction).</li>
<li>2026-02-20: Approved protocol freeze
<code>TOT-HF-P2-EPV2-2026-02-20</code> and promoted
<code>evaluation-protocol-v2.md</code> as active protocol.</li>
<li>2026-02-20: Locked protocol-v2 matrix to fixed available model set
with no within-matrix substitutions.</li>
<li>2026-02-20: Completed full 3-model protocol-v2 matrix execution and
added matrix-level summary artifacts.</li>
<li>2026-02-21: Completed evaluator ablation runs
(<code>rule_based</code>, <code>hybrid</code>) on primary model and
added consolidated evaluator-ablation summary artifacts.</li>
<li>2026-02-21: Added search-ablation execution playbook and summary
tooling (<code>build_search_ablation_summary.py</code>).</li>
<li>2026-02-21: Completed A1/A2 search-policy ablation runs on primary
model and archived consolidated search-ablation summary artifacts.</li>
<li>2026-02-21: Refreshed protocol-v2 failure taxonomy artifacts from
archived Hugging Face manifests.</li>
<li>2026-02-21: Drafted manuscript-ready Methods and Experimental Setup
text from frozen protocol artifacts.</li>
<li>2026-02-21: Drafted manuscript-ready Results and Limitations text
from frozen matrix, ablation, and taxonomy artifacts.</li>
</ul>
</body>
</html>
